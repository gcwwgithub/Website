Category,Question,Code Question,QuestionImage,Answer,Code Answer,AnswerImage,Color,WrongAnswer1,WrongAnswer2,WrongAnswer3
DSA Week 1 Day 3,What does time complexity measure in an algorithm?,-,-,How the runtime grows with the size of the input n.,-,-,-,How much memory the algorithm uses as input size increases.,The exact number of CPU instructions executed.,How fast the algorithm runs on a specific computer.
DSA Week 1 Day 3,What is the difference between time complexity and actual runtime?,-,-,Time complexity is a theoretical growth rate; actual runtime depends on hardware and implementation.,-,-,-,Time complexity always predicts the exact runtime.,"Actual runtime ignores input size, unlike time complexity.","Time complexity only applies to recursive algorithms, not iterative ones."
DSA Week 1 Day 3,What does Big-O notation represent?,-,-,The upper bound on how runtime grows — the worst-case growth rate.,-,-,-,The exact runtime of an algorithm for all inputs.,The lower bound on the number of operations required.,How long the algorithm takes on average-case inputs.
DSA Week 1 Day 3,Why do we ignore constants in Big-O analysis?,-,-,They don’t affect how runtime scales as n becomes very large.,-,-,-,Constants always stay zero in asymptotic analysis.,"Constants only matter for recursive algorithms, not iterative ones.","Constants are unknown, so we remove them arbitrarily."
DSA Week 1 Day 3,Give an example of an O(1) operation.,-,-,Accessing an array element by index.,-,-,-,Iterating through all elements of an array.,Performing a binary search on a sorted list.,Sorting an array of fixed size using a comparison sort.
DSA Week 1 Day 3,Give an example of an O(n) algorithm.,-,-,Linear search through an array.,-,-,-,Binary search on a sorted array.,Merge Sort on any input.,Checking whether an array is sorted using only recursion.
DSA Week 1 Day 3,Give an example of an O(log n) algorithm.,-,-,Binary search on a sorted array.,-,-,-,Inserting into a singly linked list at the head.,Scanning the entire array once from start to end.,Dividing the array into constant-size chunks repeatedly and scanning all chunks.
DSA Week 1 Day 3,Give an example of an O(n log n) algorithm.,-,-,Merge Sort or average-case Quick Sort.,-,-,-,Bubble Sort for any input.,Searching for an item using a hash table.,Checking if an array contains duplicates using a single linear scan.
DSA Week 1 Day 3,What is the time complexity of a nested loop where both loops go from 1 to n?,-,-,O(n²).,-,-,-,O(n).,O(log n).,O(n log n).
DSA Week 1 Day 3,What is the time complexity of a loop that halves n each iteration?,-,-,O(log n).,-,-,-,O(n).,O(1).,O(n²).
DSA Week 1 Day 3,What does n represent in Big-O notation?,-,-,The size of the input data.,-,-,-,The amount of available memory on the system.,The number of CPU cores available.,The number of variables declared in the algorithm.
DSA Week 1 Day 3,How do you determine the dominant term in an expression like 3n² + 5n + 10?,-,-,The n² term dominates — the time complexity is O(n²).,-,-,-,The constant term 10 dominates for large n.,All terms are equally important in Big-O and must be kept.,The linear term dominates because 5n > 3n² for small n.
DSA Week 1 Day 3,What’s the time complexity of a constant-time statement executed n times in a loop?,-,-,O(n).,-,-,-,O(1) because each statement is constant-time.,O(n²) because the statement is repeated.,O(log n) because loops scale logarithmically.
DSA Week 1 Day 3,What is the time complexity of checking every pair of elements in an array?,-,-,O(n²).,-,-,-,O(n).,O(log n).,O(n log n).
DSA Week 1 Day 3,What is the difference between worst-case and average-case time complexity?,-,-,Worst-case is the maximum time; average-case is the expected time over all inputs.,-,-,-,Average-case is always better than worst-case.,Worst-case only applies to sorting algorithms.,Average-case complexity ignores the input distribution.
DSA Week 1 Day 3,What is asymptotic analysis?,-,-,Studying algorithm growth rates as n → ∞ to understand scalability.,-,-,-,Measuring exact runtime for small input sizes.,"Analyzing memory usage only, ignoring time.",Comparing how different CPUs run the same compiled code.
DSA Week 1 Day 3,What is the meaning of log n in time complexity?,-,-,The number of times you can divide n by 2 before reaching 1.,-,-,-,The number of loops in the program.,The time it takes for the operating system to allocate memory.,The number of digits in the input value.
DSA Week 1 Day 3,"If one algorithm is O(n) and another is O(n²), which is faster for very large n?",-,-,The O(n) algorithm.,-,-,-,"The O(n²) algorithm, because quadratic algorithms handle larger n better.",They are equally fast for all input sizes.,The O(n²) algorithm if the constant factor is very small.
DSA Week 1 Day 3,Can an O(n²) algorithm ever run faster than an O(n) algorithm in practice?,-,-,"Yes, for small n or when the O(n) algorithm has large constant factors.",-,-,-,"No, because Big-O always predicts the exact runtime.",Only for infinitely large inputs.,Only if the CPU is specifically optimized for quadratic algorithms.
DSA Week 1 Day 3,What is the difference between Big-O and Big-Theta (Θ)?,-,-,Big-O gives an upper bound; Big-Theta gives a tight bound (both upper and lower).,-,-,-,Big-Theta is only used for best-case scenarios.,Big-O always equals Big-Theta for any correct algorithm.,Big-Theta is always larger than Big-O for the same function.
DSA Week 1 Day 3,"If f(n) ∈ O(g(n)) and f(n) ∈ Ω(g(n)), what can you conclude?",-,-,f(n) ∈ Θ(g(n)).,-,-,-,f(n) grows strictly slower than g(n).,f(n) grows strictly faster than g(n).,f(n) and g(n) are not asymptotically comparable.
DSA Week 1 Day 4,What does space complexity measure in an algorithm?,-,-,The amount of extra memory an algorithm uses relative to input size n.,-,-,-,How long an algorithm takes to run on the worst possible input.,The amount of memory taken by the compiled program on disk.,The total RAM available on the machine at runtime.
DSA Week 1 Day 4,What’s the difference between space complexity and time complexity?,-,-,"Space complexity measures memory usage, while time complexity measures execution time.",-,-,-,Space complexity measures both memory and time since memory affects caching.,"Time complexity is only relevant for recursive algorithms, not iterative ones.","Space complexity only applies to large inputs, while time complexity applies to all inputs."
DSA Week 1 Day 4,What is auxiliary space?,-,-,"Extra memory used by an algorithm beyond the input data (e.g., temporary arrays, recursion stack).",-,-,-,Memory used by the input array itself.,Memory used by the operating system to load the program.,"Memory allocated only on the heap, not the stack."
DSA Week 1 Day 4,What is input space?,-,-,The memory occupied by the input data itself.,-,-,-,The memory used to store temporary variables.,The memory used by the CPU cache during execution.,Additional memory allocated by the algorithm during recursion.
DSA Week 1 Day 4,What does O(1) space mean?,-,-,"The algorithm uses a constant amount of extra memory, regardless of input size.",-,-,-,The algorithm runs in constant time for all inputs.,The memory used decreases as input size increases.,The algorithm only uses stack memory and never heap memory.
DSA Week 1 Day 4,How does recursion affect space complexity?,-,-,"Each recursive call adds a new frame to the call stack, increasing memory usage.",-,-,-,Recursion always reduces memory usage because repeated work is avoided.,"Recursion only affects time complexity, not memory usage.",Recursive functions reuse a single stack frame for all recursive calls.
DSA Week 1 Day 4,What is the space complexity of iterative Fibonacci vs recursive Fibonacci?,-,-,"Iterative: O(1), Recursive: O(n) due to the call stack.",-,-,-,"Iterative: O(n), Recursive: O(1) because recursion is faster.",Both iterative and recursive Fibonacci always take O(n²) space.,Recursive Fibonacci uses O(log n) space because the recursion tree height is logarithmic.
DSA Week 1 Day 4,"How does dynamic memory allocation (e.g., new, malloc) affect space complexity?",-,-,It increases auxiliary space since extra memory is explicitly allocated.,-,-,-,It decreases space complexity because memory is allocated only when needed.,"It only affects time complexity, not space at all.",It has no impact unless the input size is extremely large.
DSA Week 1 Day 4,What is a space-time trade-off?,-,-,"When you use more memory to make an algorithm faster (e.g., using caching or precomputed tables).",-,-,-,When you reduce both time and space complexity simultaneously.,When time complexity is always sacrificed to reduce space complexity.,When memory is freed early to make the program finish faster.
DSA Week 1 Day 4,How does storing precomputed results (memoization) affect space complexity?,-,-,It increases space complexity to reduce time complexity.,-,-,-,It decreases both time and space complexity.,It has no effect on memory because results overwrite old ones.,"It only affects worst-case time complexity, not memory usage."
DSA Week 1 Day 4,"In recursion, what contributes most to space complexity?",-,-,The call stack holding local variables and return addresses.,-,-,-,The number of global variables declared in the program.,The CPU registers used for calculations.,The compiled binary file size on disk.
DSA Week 1 Day 4,What is the total space complexity formula?,-,-,Total space = Input space + Auxiliary space.,-,-,-,Total space = Auxiliary space × Input space.,Total space = Stack space − Heap space.,Total space = Algorithm time + Memory usage.
DSA Week 1 Day 4,Does passing parameters by value affect space usage?,-,-,"Yes — it duplicates the data being passed, consuming additional memory.",-,-,-,No — pass-by-value always reuses the same memory location.,"Only pointer types affect space usage, not value types.",Passing by value affects time but never space.
DSA Week 1 Day 4,Why is understanding space complexity important?,-,-,"To prevent memory overflow, stack overflow, and inefficient memory usage in large-scale systems.",-,-,-,Because space complexity determines how fast CPUs execute instructions.,Because using less space always guarantees faster algorithms.,Because space complexity tells you how much disk storage your code will require.
DSA Week 1 Day 5,What does Big-O notation represent?,-,-,The upper bound of an algorithm’s growth rate — the worst-case growth rate.,-,-,-,The exact number of operations the algorithm performs.,The best-case growth rate of the algorithm.,The total runtime including all constant factors.
DSA Week 1 Day 5,What does Big-Ω notation represent?,-,-,The lower bound of an algorithm’s growth rate — the minimum growth rate.,-,-,-,The average-case performance of the algorithm.,The tightest possible upper bound on runtime.,The total memory usage of the algorithm.
DSA Week 1 Day 5,What does Big-Θ notation represent?,-,-,A tight bound — the algorithm grows at that rate from both above and below.,-,-,-,A loose bound used only when Big-O cannot be determined.,The exact runtime of the algorithm on all inputs.,A notation used only for recursive algorithms.
DSA Week 1 Day 5,"What is the relationship among O, Ω, and Θ?",-,-,"For a function f(n), we have Ω(f(n)) ≤ Θ(f(n)) ≤ O(f(n)).",-,-,-,"For a function f(n), we have O(f(n)) ≤ Ω(f(n)) ≤ Θ(f(n)).",Θ(f(n)) is always equal to O(f(n)).,Ω(f(n)) only applies if f(n) is negative.
DSA Week 1 Day 5,Give an intuitive way to think about Big-O.,-,-,“At most this fast” — the algorithm won’t grow faster than O(f(n)) for large n.,-,-,-,“Exactly this fast” — Big-O predicts the precise runtime.,“At least this fast” — the algorithm will always take O(f(n)).,“Bound only by machine hardware limitations.”
DSA Week 1 Day 5,Give an intuitive way to think about Big-Ω.,-,-,“At least this fast” — the algorithm grows no slower than Ω(f(n)).,-,-,-,“At most this fast.”,“The average growth rate for all inputs.”,“The growth rate only for recursive algorithms.”
DSA Week 1 Day 5,Give an intuitive way to think about Big-Θ.,-,-,“Exactly this fast” — a tight bound on growth.,-,-,-,“A pessimistic worst-case bound.”,“A temporary bound used only during proofs.”,“A bound only valid for best-case inputs.”
DSA Week 1 Day 5,"In the expression 3n² + 5n + 10, which term dominates asymptotically?",-,-,The n² term dominates asymptotically — the function is Θ(n²).,-,-,-,The constant term 10 dominates for large n.,The linear term dominates because 5n > 3n² for small n.,No term dominates because all contribute equally for large n.
DSA Week 1 Day 5,Why do we drop constants and lower-order terms in Big-O notation?,-,-,Because asymptotic analysis focuses on growth as n → ∞; constants don’t change the scaling pattern.,-,-,-,Because constants are always equal to zero in asymptotic expressions.,"Because constants only apply to recursive algorithms, not iterative ones.",Because lower-order terms are mathematically invalid in asymptotic notation.
DSA Week 1 Day 5,How would you prove that 3n + 2 ∈ O(n)?,-,-,"Show that 3n + 2 ≤ 4n for n ≥ 2, choosing c = 4 and n₀ = 2.",-,-,-,Show that 3n + 2 = n² for sufficiently large n.,Show that 3n + 2 is always less than n for all n.,Choose c = 0 so the lower-order term disappears.
DSA Week 1 Day 5,What is the formal definition of Big-O?,-,-,f(n) = O(g(n)) if there exist constants c > 0 and n₀ > 0 such that 0 ≤ f(n) ≤ c·g(n) for all n ≥ n₀.,-,-,-,f(n) = O(g(n)) only if 0 ≤ c·g(n) ≤ f(n) for all n ≥ n₀.,f(n) = O(g(n)) if f(n) eventually equals g(n) exactly for all n.,f(n) = O(g(n)) only when both functions are polynomials.
DSA Week 1 Day 5,What is the formal definition of Big-Ω?,-,-,f(n) = Ω(g(n)) if there exist c > 0 and n₀ > 0 such that 0 ≤ c·g(n) ≤ f(n) for all n ≥ n₀.,-,-,-,f(n) = Ω(g(n)) when 0 ≤ f(n) ≤ c·g(n) for all n ≥ n₀.,f(n) = Ω(g(n)) only when f(n) grows faster than n².,f(n) = Ω(g(n)) if the algorithm is recursive.
DSA Week 1 Day 5,What is the formal definition of Big-Θ?,-,-,"f(n) = Θ(g(n)) if there exist c₁, c₂ > 0 and n₀ > 0 such that c₁·g(n) ≤ f(n) ≤ c₂·g(n) for all n ≥ n₀.",-,-,-,f(n) = Θ(g(n)) only if f(n) = g(n) exactly for all n.,f(n) = Θ(g(n)) when f(n) < g(n) for all n.,f(n) = Θ(g(n)) only if c₁ = c₂.
DSA Week 1 Day 5,What do the constants c and n₀ represent in formal definitions?,-,-,c is a scaling factor for comparison; n₀ is the point after which the relationship always holds.,-,-,-,c is the runtime of the algorithm; n₀ is the size of the array.,c controls memory usage; n₀ is the number of recursion levels.,c is the slope of the time complexity line; n₀ is the y-intercept.
DSA Week 1 Day 5,"If f(n) = 5n² + 3n, what are its O, Ω, and Θ classifications?",-,-,"O(n²), Ω(n²), and Θ(n²).",-,-,-,"O(n), Ω(n³), and Θ(n).","O(n²), Ω(n), and Θ(1).","O(1), Ω(1), and Θ(n²)."
DSA Week 1 Day 5,"If f(n) = n log n + n, what is its Big-O classification?",-,-,O(n log n).,-,-,-,O(n²).,O(log n).,O(n).
DSA Week 1 Day 5,Why can O(n²) also describe an O(n) algorithm?,-,-,Because Big-O is an upper bound — the set of O(n) functions is a subset of O(n²).,-,-,-,Because Big-O always describes the smallest possible bound.,Because n² is always smaller than n for sufficiently large n.,Because O(n²) applies only to best-case scenarios.
DSA Week 1 Day 5,What does it mean if f(n) ∈ Θ(g(n))?,-,-,f(n) and g(n) grow at the same rate asymptotically (within constant factors).,-,-,-,f(n) always grows faster than g(n) for large n.,g(n) is an upper bound but not a lower bound for f(n).,f(n) must be constant for large enough n.
DSA Week 1 Day 5,What is asymptotic dominance?,-,-,"If g(n) = O(f(n)), then f(n) dominates g(n) for large n.",-,-,-,"If f(n) = O(g(n)), then f(n) dominates g(n) for large n.",Dominance only occurs if both functions are linear.,Dominance means both functions have identical growth rates.
DSA Week 1 Day 5,Is Big-O analysis exact or approximate?,-,-,"Approximate — it describes order of growth, not exact timings.",-,-,-,Exact — it gives the precise number of operations.,Exact — it reflects the real runtime on the machine.,Exact — it ignores all lower-order terms but keeps constants.
DSA Week 1 Day 5,"If f(n) ∈ O(g(n)) and f(n) ∈ Ω(g(n)), what can you conclude?",-,-,f(n) ∈ Θ(g(n)).,-,-,-,f(n) grows slower than g(n).,f(n) grows faster than g(n).,f(n) is not comparable to g(n) asymptotically.
DSA Week 1 Day 6,What does worst-case time complexity measure?,-,-,The maximum possible time an algorithm could take for any input of size n.,-,-,-,The time the algorithm takes on the most likely input.,The exact runtime on average inputs.,The minimum time the algorithm could take.
DSA Week 1 Day 6,What does best-case time complexity measure?,-,-,The minimum time an algorithm could take for the easiest possible input.,-,-,-,The fastest runtime the hardware can achieve.,The expected runtime averaged over all inputs.,The slowest runtime for any input.
DSA Week 1 Day 6,What does average-case time complexity measure?,-,-,"The expected running time over all possible inputs, assuming a probability distribution.",-,-,-,The worst-case runtime divided by 2.,The runtime for the input the programmer thinks is typical.,The runtime of the algorithm when constants are ignored.
DSA Week 1 Day 6,Why is worst-case analysis often preferred?,-,-,It guarantees performance bounds regardless of input.,-,-,-,Worst-case always equals average-case complexity.,Worst-case only applies to recursive algorithms.,Worst-case complexity automatically gives best-case complexity too.
DSA Week 1 Day 6,Give an example of a worst-case scenario for linear search.,-,-,"The target element is not in the array, so every element must be checked.",-,-,-,The target element is the first element in the array.,The array is already sorted in ascending order.,The target value is larger than all elements in a sorted array.
DSA Week 1 Day 6,Give an example of a best-case scenario for linear search.,-,-,The target element is the first element in the array.,-,-,-,The array is in descending order.,The element appears more than once in the array.,The array contains only unique values.
DSA Week 1 Day 6,What is the average-case complexity of linear search?,-,-,"O(n), because on average half the elements are checked.",-,-,-,"O(1), because the element could be at index 0.","O(log n), because probability reduces the search space.","O(n²), because all pairs of elements might need checking."
DSA Week 1 Day 6,What is the worst-case complexity of binary search?,-,-,O(log n).,-,-,-,O(n).,O(1).,O(n log n).
DSA Week 1 Day 6,What is the average-case complexity of binary search?,-,-,O(log n).,-,-,-,"O(n), because half the array is checked.","O(1), because the pivot is in the middle.","O(n²), because recursive calls happen repeatedly."
DSA Week 1 Day 6,What is the worst-case complexity of Quick Sort?,-,-,O(n²).,-,-,-,O(log n).,O(1).,"O(n), because each pivot removes one element."
DSA Week 1 Day 6,What is the average-case complexity of Quick Sort?,-,-,O(n log n).,-,-,-,O(n²).,O(log n).,O(n) for all pivot choices.
DSA Week 1 Day 6,What is the best-case complexity of Quick Sort?,-,-,"O(n log n), when the pivot always splits the array evenly.",-,-,-,"O(1), because partitioning is constant time.","O(n²), due to recursion depth.","O(log n), because half the elements are removed each time."
DSA Week 1 Day 6,What does amortized complexity measure?,-,-,The average cost per operation over a sequence of operations.,-,-,-,The runtime averaged over many random inputs.,The worst-case runtime of a single operation.,The best-case runtime assuming perfectly ordered input.
DSA Week 1 Day 6,When is amortized analysis useful?,-,-,When most operations are cheap but some are occasionally expensive.,-,-,-,When every operation always takes the same amount of time.,When you want to guarantee the worst-case runtime of every operation.,When analyzing only recursive algorithms.
DSA Week 1 Day 6,Give an example of an algorithm with amortized O(1) operations.,-,-,Dynamic array push_back (like std::vector or List<T>).,-,-,-,Heap insertion in a binary heap.,Binary search on a sorted array.,Merge sort on an array.
DSA Week 1 Day 6,How does amortized O(1) work for dynamic array insertion?,-,-,Most inserts take O(1); resizing takes O(n) but happens rarely enough that the average cost per insert stays constant.,-,-,-,Every insert is O(1) because resizing never happens.,"Resizing is O(log n), so inserts are logarithmic.",The insert cost decreases as the array gets bigger.
DSA Week 1 Day 6,What are the three common methods of amortized analysis?,-,-,"Aggregate method, accounting method, and potential method.",-,-,-,"Recursive method, iterative method, and probabilistic method.","Lower bound, tight bound, and upper bound methods.","Divide-and-conquer, greedy, and dynamic programming methods."
DSA Week 1 Day 6,What’s the amortized complexity of inserting n elements into a dynamic array that doubles when full?,-,-,"O(n) total time, which is O(1) amortized per insertion.",-,-,-,"O(n²) total time, which is O(n) amortized per insertion.","O(log n) total time, which is O(1/log n) amortized per insertion.","O(n log n) total time, which is O(log n) amortized per insertion."
DSA Week 1 Day 6,What is the difference between average-case and amortized complexity?,-,-,Average-case is expected time over all inputs; amortized is average time over a sequence of operations on one input.,-,-,-,They are identical for all algorithms.,Amortized complexity is always worse than average-case complexity.,Average-case complexity ignores input distribution completely.
DSA Week 1 Day 6,Why is amortized analysis important for data structures?,-,-,It ensures performance stability for operations even when some individual operations are occasionally expensive.,-,-,-,It guarantees the best-case time for all operations.,It eliminates worst-case scenarios entirely.,It only applies to algorithms that use recursion.
DSA Week 1 Day 6,Give another example of amortized O(1) behavior besides dynamic arrays.,-,-,Stack push and pop operations in an array-based stack.,-,-,-,Breadth-first search on a graph.,Matrix multiplication using the standard algorithm.,Merge sort’s merging phase.
DSA Week 1 Day 6,What is the amortized complexity of enqueue and dequeue in a two-stack queue implementation?,-,-,O(1) amortized for both enqueue and dequeue.,-,-,-,O(n²) amortized for both operations.,O(log n) amortized for both operations.,O(n log n) amortized for both operations.
DSA Week 1 Day 6,Which type of complexity is best for guaranteeing real-time performance?,-,-,Worst-case complexity.,-,-,-,Best-case complexity.,Amortized complexity.,Average-case complexity only.
DSA Week 1 Day 6,Which type of complexity is most realistic for expected behavior in general-purpose apps?,-,-,Average-case complexity.,-,-,-,Worst-case complexity.,Best-case complexity.,"Amortized complexity, regardless of workload."
DSA Week 1 Day 6,"Which type of complexity gives a balanced, long-term cost view for repeated operations?",-,-,Amortized complexity.,-,-,-,Best-case complexity.,Space complexity only.,Worst-case complexity because it’s pessimistic.
DSA Week 1 Day 6,What does the aggregate method mean in amortized analysis?,-,-,Total cost of n operations divided by n gives the amortized cost per operation.,-,-,-,The sum of the worst-case costs only.,The maximum cost of any single operation in the sequence.,Counting only the cheapest operations in the sequence.
DSA Week 1 Day 6,What’s the amortized cost of doubling array capacity each time it’s full?,-,-,O(1) amortized per insertion.,-,-,-,O(n²) per insertion.,O(log n) per insertion.,O(n) amortized per insertion.
DSA Week 1 Day 6,What is the main takeaway from amortized analysis?,-,-,Occasional expensive operations don’t break overall efficiency if they are sufficiently rare.,-,-,-,Worst-case time becomes irrelevant in all analyses.,All operations must be constant-time in the worst case.,The algorithm must be recursive to use amortized analysis.
